{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script is intended for variable importance identification and error metrics recording. Enter variables being tested in the cell below and re-run the entire script.\n",
    "#### emis:\n",
    "ACET            ACROLEIN        ALD2            ALD2_PRIMARY    ALDX            APIN            BENZ            BPIN            BUTADIENE13     CH4             CH4_INV         CL2             CO              CO2_INV         ETH             ETHA            ETHY            ETOH            FORM            FORM_PRIMARY    HCL             HONO            IOLE            ISOP            KET             MEOH            N2O_INV         NAPH            NH3             NH3_FERT        NO              NO2             NR              NVOL            OLE             PAL             PAR             PCA             PCL             PEC             PFE             PH2O            PK              PMC             PMG             PMN             PMOTHR          PNA             PNCOM           PNH4            PNO3            POC             PRPA            PSI             PSO4            PTI             SESQ            SO2             SOAALK          SULF            TERP            TOL             UNK             UNR             VOC_BEIS        VOC_INV         XYLMN\n",
    "\n",
    "#### met: \n",
    "PRSFC           USTAR           WSTAR           PBL             ZRUF            MOLI            HFX             LH              RADYNI          RSTOMI          TEMPG           TEMP2           Q2              WSPD10          WDIR10          GLW             GSW             RGRND           RN              RC              CFRAC           CLDT            CLDB            WBAR            SNOCOV          VEG             LAI             SEAICE          SNOWH           WR              SOIM1           SOIM2           SOIT1           SOIT2           SLTYP           WSAT_PX         WFC_PX          WWLT_PX         CSAND_PX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emis_variables = ['NO2','VOC_INV']\n",
    "met_variables = ['TEMP2', 'RN'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM\n",
    "from keras.utils import np_utils\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "analyze = False\n",
    "save_to_file = False\n",
    "read_from_file = False\n",
    "create_plots = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_file_list(data_dir):\n",
    "    return [os.path.join(data_dir, name) for name in os.listdir(data_dir)]\n",
    "\n",
    "\n",
    "aq_dir='C:/Users/woshi/Desktop/CE_675_Project/Data/aq_conc/'\n",
    "emis_dir='C:/Users/woshi/Desktop/CE_675_Project/Data/emis_data/'\n",
    "met_dir='C:/Users/woshi/Desktop/CE_675_Project/Data/met_data/'\n",
    "\n",
    "aq_files = get_file_list(aq_dir)\n",
    "emis_files = get_file_list(emis_dir)\n",
    "met_files = get_file_list(met_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Input Data\n",
    "conc_data = netCDF4.MFDataset(aq_files)#concentrations \n",
    "o3_data = conc_data['O3']\n",
    "emis_data = netCDF4.MFDataset(emis_files) #emissions \n",
    "met_data = netCDF4.MFDataset(met_files)#Met\n",
    "\n",
    "#Monitor_Locations\n",
    "monitors_file = 'C:/Users/woshi/Desktop/CE_675_Project/Data/loc_data/monitor_list_NC.csv'\n",
    "with open(monitors_file) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    monitor_points = [(int(float(row['Col'])), int(float(row['Row'] ))) for row in reader if row['Col']]\n",
    "\n",
    "monitors_x, monitors_y = zip(*monitor_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "training_params = {\n",
    "                   'd' : 1, #local_emissions_size\n",
    "                   'emis_species': emis_variables, #emission_species\n",
    "                   'met_params':met_variables, #met parameters\n",
    "                   'emis_data': emis_data,\n",
    "                   'met_data':met_data,\n",
    "                   'conc_data': o3_data,\n",
    "                   'monitor_points':monitor_points\n",
    "                 }\n",
    "## Random points for testing/performance evaluation\n",
    "np.random.seed(1977)\n",
    "num_points = 300\n",
    "\n",
    "#No. of periods in each training sequence\n",
    "sequence_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_random_domain_points(num_points, domain_params):\n",
    "    x_points = np.random.normal(domain_params['x_center'], \n",
    "                                domain_params['x_std'],\n",
    "                                num_points)\n",
    "    \n",
    "    y_points = np.random.normal(domain_params['y_center'], \n",
    "                                domain_params['y_std'],\n",
    "                               num_points)\n",
    "\n",
    "    def threshold_points(x,maxx,minn):\n",
    "        return x[(x<maxx) & (x>minn)]\n",
    "\n",
    "    x_p = map(int,threshold_points(x_points,domain_params['x_max'], domain_params['x_min']))\n",
    "    y_p = map(int,threshold_points(y_points,domain_params['y_max'], domain_params['y_min']))\n",
    "    return list(set(zip(x_p, y_p)))\n",
    "    \n",
    "\n",
    "\n",
    "domain_params = {\n",
    "    'x_center': 65, 'x_std' : 15, 'x_max': 180, 'x_min' : 0,\n",
    "    'y_center' : 125,'y_std' : 15,'y_max': 180, 'y_min':0    \n",
    "}\n",
    "\n",
    "if not read_from_file:\n",
    "    print('Generating random points for training and testing')\n",
    "    pxy = get_random_domain_points(num_points,domain_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dist_predictors(x,y, monitor_points,centroid=True,**kwargs):\n",
    "    \n",
    "    x_centroid = np.average([point[0] for point in monitor_points])\n",
    "    y_centroid = np.average([point[1] for point in monitor_points])\n",
    "   \n",
    "    def unit_vector(vector):\n",
    "        return vector / np.linalg.norm(vector)\n",
    "\n",
    "    def angle_between(v1, v2):\n",
    "        v1_u = unit_vector(v1)\n",
    "        v2_u = unit_vector(v2)\n",
    "        return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "    if centroid:   \n",
    "        distance = math.sqrt((x_centroid - x)**2 + (y_centroid -y)**2)\n",
    "        v1 = (x_centroid, y_centroid)\n",
    "        v2 = (x,y)\n",
    "        angle = angle_between(v1,v2)\n",
    "        return np.array([distance, angle])\n",
    "    else:\n",
    "        \n",
    "        distances =[math.sqrt((x - point[0])**2 + (y - point[1])**2) for point in monitor_points]\n",
    "        angles = [angle_between((x,y), (point[0], point[1])) for point in monitor_points]\n",
    "        da = distances + angles\n",
    "        return np.array(da) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     17
    ]
   },
   "outputs": [],
   "source": [
    "def emis_predictors(x,y,tlist, d, emis_data, emis_species, **kwargs):\n",
    "\n",
    "    xs = np.arange(-d,d+1) + x\n",
    "    ys = np.arange(-d,d+1) + y\n",
    "    num_cells = len(xs)*len(ys)\n",
    "\n",
    "    dummy = np.arange(1)\n",
    "    grid = np.ix_(tlist,dummy,xs,ys)\n",
    "    \n",
    "    def get_emis_slice(emis_data,species):\n",
    "        e1 = emis_data[species][:]\n",
    "        e2 = e1[grid]\n",
    "        return e2.reshape(len(tlist), num_cells)\n",
    "\n",
    "    emis_seq = [ get_emis_slice(emis_data, species) for species in emis_species] \n",
    "    return np.concatenate(emis_seq,axis=1)    \n",
    "\n",
    "def history_monitors_predictors(t,conc_data, monitor_points, **kwargs):\n",
    "\n",
    "    monitors_x, monitors_y = zip(*monitor_points) \n",
    "    concs = conc_data[t][:,0,monitors_x,monitors_y]\n",
    "\n",
    "    return concs\n",
    "\n",
    "def met_predictors(t,monitor_points, met_data, met_params, **kwargs):\n",
    "   \n",
    "    monitors_x, monitors_y = zip(*monitor_points) \n",
    "    \n",
    "    met_seq = []\n",
    "    for param in met_params:\n",
    "        m1 = met_data[param][:]\n",
    "        m2 = m1[t][:,0,monitors_x,monitors_y]\n",
    "        #print(m2)\n",
    "        #print(m2.shape)\n",
    "        met_seq.append(m2)\n",
    "    return np.concatenate(met_seq, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_predictors_xyt(p,tlist,training_params,debug=False):\n",
    "    \n",
    "    x = p[0]\n",
    "    y = p[1]\n",
    "\n",
    "    dist = dist_predictors(x,y,**training_params) #time invariant\n",
    "    dist_tile = np.tile(dist,(len(tlist),1))\n",
    "\n",
    "    emis = emis_predictors(x,y,tlist,**training_params)\n",
    "    met  = met_predictors(tlist,**training_params)\n",
    "    hist = history_monitors_predictors(tlist,**training_params)\n",
    "    \n",
    "    try:\n",
    "        preds = np.concatenate([dist_tile, emis, hist,met], axis=1)\n",
    "        return preds\n",
    "    except:\n",
    "        return [dist_tile,emis,hist,met]\n",
    "\n",
    "def get_Xy(pxy, hour_begin_seq, training_params):\n",
    "    from itertools import product, repeat\n",
    "\n",
    "    predictors = {}\n",
    "    aq = {}\n",
    "    conc_data =training_params['conc_data']\n",
    "    pxy_time = [i for i in product(pxy,hour_begin_seq)]\n",
    "    for pt in tqdm(pxy_time, desc='Seq Processed'):\n",
    "        point = pt[0]\n",
    "        hour_begin = pt[1]\n",
    "        tlist = [i for i in range(hour_begin - sequence_size, hour_begin)]\n",
    "        predictors[pt] = get_predictors_xyt(point,tlist,training_params,debug=False)\n",
    "        \n",
    "        hour_next = hour_begin + 1\n",
    "        aq[pt] = conc_data[hour_next, 0, point[0], point[1]]\n",
    "    \n",
    "    return predictors, aq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for times in range(1,11):\n",
    "    hour_begin_seq_split = [0,6,12,18,24,30,36,42,48,54,60]\n",
    "\n",
    "    if not read_from_file:\n",
    "        predictors, aq = get_Xy(pxy, hour_begin_seq_split,training_params)    \n",
    "        print ('\\n Done getting predictors')\n",
    "\n",
    "    def get_splits(predictors,train_fraction=0.95):\n",
    "        num_predictors = len(predictors)\n",
    "        pxy = list(predictors.keys())\n",
    "\n",
    "        num_train = int(train_fraction*num_predictors)\n",
    "        train_indices = np.random.choice(np.arange(num_predictors),num_train, replace=False)\n",
    "        pxy_train = [pxy[i] for i in train_indices]\n",
    "        predictors_train = [predictors[i] for i in pxy_train]\n",
    "        aq_train = [aq[i] for i in pxy_train]\n",
    "        print ('number of training points', num_train, '&', len(predictors_train))\n",
    "\n",
    "        #Random sample for Testing\n",
    "        predictors_test = [predictors[i] for i in pxy if i not in pxy_train]\n",
    "        aq_test = [aq[i] for i in pxy if i not in pxy_train]\n",
    "        pxy_test = [i for i in pxy if i not in pxy_train]\n",
    "        print('number of testing points', len(predictors_test), '&', len(aq_test))\n",
    "        return predictors_train, aq_train, predictors_test, aq_test\n",
    "\n",
    "    if not read_from_file:\n",
    "        print('Getting train and test splits')\n",
    "        predictors_train, aq_train, predictors_test, aq_test = get_splits(predictors)\n",
    "\n",
    "    if not read_from_file:\n",
    "\n",
    "        print('scaling inputs and outputs')\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "        data = np.concatenate(list(predictors.values()))\n",
    "        pred_scaler = MinMaxScaler()\n",
    "        pred_scaler.fit(data)\n",
    "\n",
    "        aq_data = np.stack(aq.values()).reshape(-1,1)\n",
    "        aq_scaler = MinMaxScaler()\n",
    "        aq_scaler.fit(aq_data)\n",
    "\n",
    "        predictors_train_scaled = [pred_scaler.transform(mx) for mx in predictors_train]\n",
    "        predictors_train_rnn = np.stack(predictors_train_scaled)\n",
    "        aq_train_rnn = aq_scaler.transform(np.stack(aq_train).reshape(-1,1))\n",
    "\n",
    "        predictors_test_scaled = [pred_scaler.transform(mx) for mx in predictors_test]\n",
    "        predictors_test_rnn = np.stack(predictors_test_scaled)\n",
    "        aq_test_rnn = aq_scaler.transform(np.stack(aq_test).reshape(-1,1))\n",
    "\n",
    "    ### Save for future use.\n",
    "    predictors_file = 'C:/Users/woshi/Desktop/CE_675_Project/Data/saved_data/'\n",
    "\n",
    "    if save_to_file == True:\n",
    "        #data_to_save = [predictors_train_rnn, aq_train, predictors_test_rnn,aq_test] \n",
    "\n",
    "        np.savez(predictors_file, predictors_train_rnn = predictors_train_rnn, #X_train, shape=(samples,tsteps,features)\n",
    "                                aq_train_rnn = aq_train_rnn, #y_train\n",
    "                                predictors_test_rnn = predictors_test_rnn, #X_test\n",
    "                                aq_test_rnn = aq_test_rnn #y_test\n",
    "                )\n",
    "    s0,s1,s2=predictors_train_rnn.shape\n",
    "    print ('shape of x_train data', s0,s1,s2)\n",
    "    print ('shape of y_train data', aq_train_rnn.shape)\n",
    "    # Design Network\n",
    "    m = Sequential()\n",
    "    m.add(LSTM(128,input_shape=predictors_train_rnn.shape[1:], return_sequences=True, activation='tanh'))\n",
    "    m.add(LSTM(128, activation='tanh'))\n",
    "    m.add(Dropout(0.50))\n",
    "    m.add(Dense(128, activation='tanh'))\n",
    "    m.add(Dense(1))\n",
    "    m.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    history = m.fit(predictors_train_rnn, aq_train_rnn, epochs=100, \n",
    "                        validation_data=(predictors_test_rnn, aq_test_rnn), verbose=2, shuffle=True\n",
    "                )\n",
    "    sns.set_style('ticks')\n",
    "    plt.rcParams['axes.linewidth'] = 5\n",
    "\n",
    "    fig = plt.figure(figsize=(14,10))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    p1 = ax.plot(history.history['loss'], linewidth=4, color ='green')\n",
    "    p2 = ax.plot(history.history['val_loss'], linewidth=4, color ='red')\n",
    "\n",
    "    ax.set_ylabel('loss (a measure of error)', fontdict={'fontsize':25})\n",
    "    ax.set_xlabel('No. of training epochs', fontdict={'fontsize':25})\n",
    "    ax.set_title('Training and Validation Loss'+str(emis_variables)+str(met_variables), fontdict={'fontsize':30}, y=1.08)\n",
    "    ax.tick_params(axis='both', labelsize=30)\n",
    "    ax.grid(axis='both', linewidth=3)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('C:/Users/woshi/Desktop/CE_675_Project/importance_identification/trn_val_loss'+\n",
    "                str(emis_variables)+str(met_variables)+'.pdf', format='pdf', dpi=1200)\n",
    "    test_predict = m.predict(predictors_test_rnn)\n",
    "\n",
    "    test_predict_descaled = aq_scaler.inverse_transform(test_predict)\n",
    "    aq_test_rnn_descaled = aq_scaler.inverse_transform(aq_test_rnn)\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    plt.rcParams['axes.linewidth'] = 5\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    p1 = ax.scatter(aq_test_rnn_descaled, test_predict_descaled, edgecolors='r', facecolor=\"none\", s=500, linewidths=1)\n",
    "    ax.set_xlim([0.02,0.06])\n",
    "    ax.set_ylim([0.02,0.06])\n",
    "    ax.set_ylabel('Predicted Concentrations \\n from Deep Learning Model (PPM)', fontdict={'fontsize':25}, labelpad=10)\n",
    "    ax.set_xlabel('Concentration from CMAQ Model\\n (PPM)', fontdict={'fontsize':25}, labelpad=20)\n",
    "    ax.set_title('Model Predictions vs Observed on Validation Dataset\\n'+str(emis_variables)+'\\n'+str(met_variables)\n",
    "                , fontdict={'fontsize':25},y=1.08)\n",
    "\n",
    "    ax.plot([0.01,0.09], [0.01,0.09], linewidth=4)\n",
    "    ax.tick_params(axis='both', labelsize=25)\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('C:/Users/woshi/Desktop/CE_675_Project/importance_identification/pdct_val'+\n",
    "                str(emis_variables)+str(met_variables)+'.pdf', format='pdf', dpi=1200)\n",
    "    pxy_future = pxy = get_random_domain_points(50,domain_params)\n",
    "    hour_begin_seq = [66,72,78,84]\n",
    "        \n",
    "    if not read_from_file:\n",
    "        predictors_future, aq_future = get_Xy(pxy, hour_begin_seq, training_params)    \n",
    "        print ('\\n Done getting predictors')\n",
    "\n",
    "    ## Scale, Predict, and Descale:\n",
    "    def get_model_predictions(model, X_as_dict, X_scaler, y_scaler):\n",
    "        \n",
    "        X = list(X_as_dict.values())\n",
    "        X_temp = [X_scaler.transform(mx) for mx in X]\n",
    "        X_scaled = np.stack(X_temp)\n",
    "        y_scaled = model.predict(X_scaled)\n",
    "        return y_scaler.inverse_transform(y_scaled)\n",
    "\n",
    "    y_model_predicted = get_model_predictions(m, predictors_future, pred_scaler, aq_scaler)\n",
    "    y = np.stack(aq_future.values()).reshape(-1,1)\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    plt.rcParams['axes.linewidth'] = 5\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    p1 = ax.scatter(y[0:50], y_model_predicted[0:50], edgecolors='r', facecolor=\"none\", s= 500, linewidths=1)\n",
    "    ax.set_xlim([0.00,0.08])\n",
    "    ax.set_ylim([0.00,0.08])\n",
    "    ax.set_ylabel('Predicted Concentrations \\n from Deep Learning Model (PPM)', fontdict={'fontsize':25}, labelpad=20)\n",
    "    ax.set_xlabel('Concentration from CMAQ Model\\n (PPM)', fontdict={'fontsize':25}, labelpad=20)\n",
    "    ax.set_title('Model Predictions vs Observed on Test Dataset \\n'+str(emis_variables) +'\\n' +str(met_variables)\n",
    "                , fontdict={'fontsize':25},y=1.08)\n",
    "\n",
    "    ax.plot([0.00,0.09], [0.00,0.09], linewidth=4)\n",
    "    ax.tick_params(axis='both', labelsize=25)\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig('C:/Users/woshi/Desktop/CE_675_Project/importance_identification/pdct_test'+\n",
    "                str(emis_variables)+str(met_variables)+'.pdf', format='pdf', dpi=1200)\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(test_predict_descaled, aq_test_rnn_descaled)\n",
    "    print('MAE is {:.2} PPM'.format(MAE))\n",
    "\n",
    "    MSE = mean_squared_error(test_predict_descaled, aq_test_rnn_descaled)\n",
    "    #print('MSE is {} PPM'.format(MSE))\n",
    "\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    print('RMSE is {:.2} PPM'.format(RMSE))\n",
    "\n",
    "    Percentage = RMSE/aq_test_rnn_descaled.mean()\n",
    "    print('Percent of RMSE to Mean AQ {:.1%}'.format(Percentage))\n",
    "\n",
    "    RMSE_test = np.sqrt(mean_squared_error(y, y_model_predicted))\n",
    "\n",
    "    import datetime\n",
    "    ISOTIMEFORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "    theTime = datetime.datetime.now().strftime(ISOTIMEFORMAT)\n",
    "    this_results = [emis_variables, met_variables, hour_begin_seq_split, len(hour_begin_seq_split), MAE, RMSE, Percentage, RMSE_test, theTime]\n",
    "    record = open('C:/Users/woshi/Desktop/CE_675_Project/importance_identification/record.csv', \"a\", newline='')\n",
    "    writer = csv.writer(record)\n",
    "    writer.writerow(this_results)\n",
    "    record.close()\n",
    "\n",
    "    times += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "745px",
    "left": "1512px",
    "right": "20px",
    "top": "48px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}