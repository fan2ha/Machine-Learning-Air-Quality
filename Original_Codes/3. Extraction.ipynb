{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import netCDF4 as ncf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from joblib import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[2,4,7,9,13,17,19]        #a is the hour for which we're trying to predict\n",
    "b=[0,1,2,3,4,5,6]      # b is indexing for a\n",
    "\n",
    "#### dates generator. 6 weeks.\n",
    "no_files=365\n",
    "nfiles_train=[]\n",
    "nfiles_test=[]\n",
    "day_count=180\n",
    "no_of_months=3\n",
    "for i in range(no_of_months):\n",
    "    nfiles_temp=list(range(day_count,day_count+15))  #7\n",
    "    nfiles_train.append(nfiles_temp)\n",
    "    nfiles_temp=list(range(day_count+16,day_count+22))  #11\n",
    "    nfiles_test.append(nfiles_temp)\n",
    "    day_count=day_count+30\n",
    "####\n",
    "##  Grid reference both x-axis,y-axis\n",
    "loc_y = [165,121,171]\n",
    "loc_x = [67,58,39]\n",
    "\n",
    "grid_size_x = dx = 20\n",
    "grid_size_y = dy = 20\n",
    "\n",
    "grid_sample_limit_x=[]\n",
    "grid_sample_limit_y=[]\n",
    "for i in loc_x:\n",
    "    grid_sample_limit_x.append((i-15,i+15))\n",
    "for i in loc_y:\n",
    "    grid_sample_limit_y.append((i-15,i+15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids_x = [np.arange(x-dx, x+dx, 1) for x in loc_x]\n",
    "grids_y = [np.arange(y-dy, y+dy, 1) for y in loc_y]\n",
    "\n",
    "all_grids_x = np.concatenate(grids_x)\n",
    "all_grids_y = np.concatenate(grids_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitors_file = 'C:/Users/woshi/Desktop/CE_675_Project/Data/loc_data/monitor_list_NC.csv'\n",
    "with open(monitors_file) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    monitor_points = [(int(float(row['Col'])), int(float(row['Row'] ))) for row in reader if row['Col']]\n",
    "\n",
    "monitors_x, monitors_y = zip(*monitor_points)\n",
    "all_grids_x=np.concatenate([all_grids_x,monitors_x])\n",
    "all_grids_y=np.concatenate([all_grids_y,monitors_y])\n",
    "all_points = list(product(all_grids_x, all_grids_y))\n",
    "\n",
    "\n",
    "met_dir='C:/Users/woshi/Desktop/CE_675_Project/Data/met_data/'\n",
    "emis_dir='C:/Users/woshi/Desktop/CE_675_Project/Data/emis_data/'\n",
    "aq_dir='C:/Users/woshi/Desktop/CE_675_Project/Data/aq_conc/'\n",
    "short_dir='C:/Users/woshi/Desktop/CE_675_Project/Data/short_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(data_dir):\n",
    "    return [os.path.join(data_dir, name) for name in os.listdir(data_dir)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(grid,val): # get the indices\n",
    "    try:\n",
    "        return np.where(grid==val)[0][0]\n",
    "    except (TypeError,IndexError):\n",
    "            if(len(val)>1):\n",
    "                index_list=[]\n",
    "                for i in val:\n",
    "                    index_list.append(np.where(grid==i)[0][0])\n",
    "                return index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_dir='C:/Users/woshi/Desktop/CE_675_Project/Data/short_data/'\n",
    "\n",
    "short_files = get_file_list(short_dir)\n",
    "\n",
    "a=[2,4,5,7,9,13,15,17,19,23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_seg(n,look_back,lb_ndarr):\n",
    "    t_part_list=[]\n",
    "    i=0\n",
    "    tlist=lb_ndarr[n]\n",
    "    def creating_tlist(tlist,j,ind):\n",
    "        try:\n",
    "            ind=tlist.index(0-j*24)\n",
    "            t_part_list.append([i+(0+j*24) for i in tlist[ind:]])\n",
    "            tlist=tlist[:ind]\n",
    "            j=j+1\n",
    "            return creating_tlist(tlist,j,ind)\n",
    "        except ValueError:\n",
    "            t_part_list.append([i+(0+j*24) for i in tlist[:ind]])\n",
    "            return t_part_list\n",
    "    t_part_list=creating_tlist(tlist,i,0)\n",
    "    return t_part_list #returns the required sets of indices for a particular hour at request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_list(a,l_b,f_h):\n",
    "    lb_ndarr = []\n",
    "    for i in a:\n",
    "        tlist=list(range(i-l_b,i))           # this is where first_ndarr is created\n",
    "        lb_ndarr.append(tlist)\n",
    "    fh_ndarr=[] #this is the list of all indices for the future data collection\n",
    "    for i in a:\n",
    "        tlist=list(range(i+1,i+f_h+1))\n",
    "        fh_ndarr.append(tlist)\n",
    "    return lb_ndarr,fh_ndarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_seg_future(n,f_h,fh_ndarr):    #if n=2; a[2]=4; tlist=5:29; firstpart=5:23;lastpart=0:5\n",
    "    t_part_list=[]\n",
    "    i=1\n",
    "    tlist=fh_ndarr[n]\n",
    "    def creating_tlist(tlist,j,ind):\n",
    "            try:\n",
    "                ind=tlist.index(0+j*24)\n",
    "                t_part_list.append([i-(0+j*24) for i in tlist[:ind]])\n",
    "                tlist=tlist[ind:]\n",
    "                \n",
    "                j=j+1\n",
    "                return creating_tlist(tlist,j,ind)\n",
    "            except ValueError:\n",
    "                t_part_list.append([i-(0+j*24) for i in tlist[:24-ind]])\n",
    "                return t_part_list\n",
    "    t_part_list=creating_tlist(tlist,i,0)\n",
    "    return t_part_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[i for i in range(len(a))]      # b is indexing for a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit(nopython=True)\n",
    "def dist_predictors(x,y, monitor_points):\n",
    "    \"\"\"\n",
    "    Returns distance and angle for an arbitrary point in the domain\n",
    "    w.r.t. monitor network. These are w.r.t. each monitor or centroid of all monitors\n",
    "    \n",
    "    Args:\n",
    "        x: x_cooridate of arbitrary point\n",
    "        y: y_coordinate of arbitrary point\n",
    "        training_params: Training parameters containing locations of monitors\n",
    "        centroid: if True, returns distance and angle from the centrod of the monitors\n",
    "    \n",
    "    Returns:\n",
    "        Angles and distances \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    centroid=True\n",
    "    x_centroid = np.average([point[0] for point in monitor_points])\n",
    "    y_centroid = np.average([point[1] for point in monitor_points])\n",
    "    \n",
    "    def unit_vector(vector):\n",
    "        \"\"\" Returns the unit vector of the vector.\"\"\"\n",
    "        return vector / np.linalg.norm(vector)\n",
    "\n",
    "    def angle_between(v1, v2):\n",
    "        \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "                >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "                1.5707963267948966\n",
    "                >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "                0.0\n",
    "                >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "                3.141592653589793\n",
    "        \"\"\"\n",
    "        v1_u = unit_vector(v1)\n",
    "        v2_u = unit_vector(v2)\n",
    "        return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "    if centroid:   \n",
    "        distance = math.sqrt((x_centroid - x)**2 + (y_centroid -y)**2)\n",
    "        v1 = (x_centroid, y_centroid)\n",
    "        v2 = (x,y)\n",
    "        angle = angle_between(v1,v2)\n",
    "        return np.array([distance, angle])\n",
    "    else:\n",
    "        \n",
    "        distances =[math.sqrt((x - point[0])**2 + (y - point[1])**2) for point in monitor_points]\n",
    "        angles = [angle_between((x,y), (point[0], point[1])) for point in monitor_points]\n",
    "        da = distances + angles\n",
    "        return np.array(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit(nopython=True)\n",
    "def emis_predictors(x,y,i_f_l,tlist):\n",
    "    d=5\n",
    "    xs = np.arange(-d,d+1) + x\n",
    "    ys = np.arange(-d,d+1) + y\n",
    "\n",
    "    num_cells = len(xs)*len(ys)\n",
    "    xs_indexed=get_index(all_grids_x,xs)\n",
    "    ys_indexed=get_index(all_grids_y,ys)\n",
    "    \n",
    "    def get_emis_slice(emis_dataset,species,tlist):\n",
    "            grid = np.ix_(tlist,xs_indexed,ys_indexed)\n",
    "            e1 = emis_dataset[species][:]\n",
    "            e2 = e1[grid]\n",
    "            return e2.reshape(len(tlist), num_cells)\n",
    "    for i in range(len(i_f_l)):\n",
    "      emis_dataset=train_param(i_f_l[i])['emis_dataset']\n",
    "      emis_species=train_param(i_f_l[i])['emis_species']\n",
    "      if(i==0):\n",
    "        emis_seq = [ get_emis_slice(emis_dataset, species,tlist[i]) for species in emis_species] \n",
    "        emis_seq = np.concatenate(emis_seq,axis=1)\n",
    "        emis_seq_total=emis_seq\n",
    "      else:\n",
    "          emis_seq = [ get_emis_slice(emis_dataset, species,tlist[i]) for species in emis_species] \n",
    "          emis_seq = np.concatenate(emis_seq,axis=1)\n",
    "          emis_seq_total=np.vstack((emis_seq_total,emis_seq))\n",
    "    return emis_seq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_monitors_predictors(t,i_f_l, monitor_points):\n",
    "\n",
    "    monitors_x, monitors_y = zip(*monitor_points) \n",
    "\n",
    "    monitors_x_edit=get_index(all_grids_x,monitors_x)\n",
    "    monitors_y_edit=get_index(all_grids_y,monitors_y)\n",
    "\n",
    "    for i in range(len(i_f_l)):\n",
    "        aq_input_data=train_param(i_f_l[i])['aq_input_data']  \n",
    "        if(i==0):\n",
    "          concs = aq_input_data['O3'][t[i]][:,monitors_x_edit,monitors_y_edit]\n",
    "          concs_total=concs\n",
    "        else:\n",
    "          concs = aq_input_data['O3'][t[i]][:,monitors_x_edit,monitors_y_edit]\n",
    "          concs_total=np.vstack((concs_total,concs))  \n",
    "    return concs_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def met_predictors(t,i_f_l,monitor_points):\n",
    "   \n",
    "    monitors_x, monitors_y = zip(*monitor_points)     \n",
    "    monitors_x_edit=get_index(all_grids_x,monitors_x)    #function to get index\n",
    "    monitors_y_edit=get_index(all_grids_y,monitors_y)    #function to get index\n",
    "    \n",
    "    def get_met(met_dataset,met_params,t):\n",
    "      met_seq = []\n",
    "      for param in met_params:\n",
    "          m1 = met_dataset[param][:]\n",
    "          m2 = m1[t][:,monitors_x_edit,monitors_y_edit]\n",
    "          met_seq.append(m2)\n",
    "      return met_seq\n",
    "    for i in range(len(i_f_l)):\n",
    "        met_dataset=train_param(i_f_l[i])['met_dataset']\n",
    "        met_params=train_param(i_f_l[i])['met_params']\n",
    "        if(i==0):\n",
    "            met_seq = get_met(met_dataset,met_params,t[i])\n",
    "            met_seq=np.concatenate(met_seq, axis=1)\n",
    "            met_seq_total=met_seq\n",
    "        else:\n",
    "            met_seq = get_met(met_dataset,met_params,t[i])\n",
    "            met_seq=np.concatenate(met_seq, axis=1)\n",
    "            met_seq_total=np.vstack((met_seq_total,met_seq))\n",
    "    return met_seq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictors_xyt(p,tlist,i_f_l,debug=False):\n",
    "    \"\"\"\n",
    "    For each point and timestamp(s) generate a row of predictors\n",
    "    \"\"\"\n",
    "    \n",
    "    x=p[0]\n",
    "    y=p[1]\n",
    "    dist = dist_predictors(x,y,monitor_points) #time invariant\n",
    "    dist_tile = np.tile(dist,(sum([len(i) for i in tlist]),1))\n",
    "\n",
    "    \n",
    "    emis = emis_predictors(x,y,i_f_l,tlist)\n",
    "    met  = met_predictors(tlist,i_f_l,monitor_points)\n",
    "    \n",
    "    hist = history_monitors_predictors(tlist,i_f_l,monitor_points)\n",
    "   \n",
    "    try:\n",
    "        preds = np.concatenate([dist_tile, emis, hist,met], axis=1)\n",
    "        return preds\n",
    "    except:\n",
    "        return [dist_tile,emis,hist,met]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aq_input(x,y,t,i_f_l):\n",
    "    x_indexed=get_index(all_grids_x,x)\n",
    "    y_indexed=get_index(all_grids_y,y)\n",
    "    \n",
    "    for i in range(len(i_f_l)):\n",
    "        #print(len(i_f_l))\n",
    "        #print(len(t))\n",
    "        \n",
    "        aq_input_data=train_param(i_f_l[i])['aq_input_data']  \n",
    "        if(i==0):\n",
    "          aq_input = aq_input_data['O3'][t[i]][:,x_indexed,y_indexed]\n",
    "          aq_input_total=aq_input\n",
    "        else:\n",
    "          aq_input = aq_input_data['O3'][t[i]][:,x_indexed,y_indexed]\n",
    "          aq_input_total=np.concatenate((aq_input_total,aq_input))  \n",
    "    return aq_input_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_future_data(o_f_l,tlist,point_x,point_y):\n",
    "    for i in range(len(o_f_l)):\n",
    "      aq_output_data=train_param(o_f_l[i])['o3_dataset']\n",
    "      if(i==0):\n",
    "        aq_output=aq_output_data['O3'][tlist[i],point_x,point_y]\n",
    "        aq_output_total=aq_output\n",
    "        #print(len(aq_output))\n",
    "      else:\n",
    "        aq_output=aq_output_data['O3'][tlist[i],point_x,point_y]\n",
    "        print(aq_output_total)\n",
    "        print(aq_output)\n",
    "        aq_output_total=np.concatenate((aq_output_total,aq_output))\n",
    "    \n",
    "    return aq_output_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_param(file_num):\n",
    "    infile=np.load(short_dir+str(file_num)+'.npz')\n",
    "    \n",
    "\n",
    "        #print(i)\n",
    "    \n",
    "    #outfile=np.load(short_dir+str(out_file_list[0])+'.npz')\n",
    "    #outfile=outfile['O3']\n",
    "    #f_output=tempfile.TemporaryFile()\n",
    "    #for i in range(1,len(out_file_list)):\n",
    "    #    file_1=np.load(short_dir+str(out_file_list[i])+'.npz')\n",
    "    #    np.savez(f_output,O3=create_npz(file_1,outfile,'O3'))\n",
    "    #    _ = f_output.seek(0) # Only needed here to simulate closing & reopening file\n",
    "    #    outfile=np.load(f_output)\n",
    "    #f_output = tempfile.NamedTemporaryFile(delete=True)\n",
    "    #f_output.close()\n",
    "    training_params = {\n",
    "                   'd' : 5, #local_emissions_size\n",
    "                   'emis_species': ['NO', 'NO2'], #emission_species\n",
    "                   'met_params':['PBL', 'Q2', 'TEMP2', 'WSPD10', 'WDIR10'], #met parameters\n",
    "                   #'met_params': ['PBL', 'WDIR10'],\n",
    "                   'emis_dataset': infile,\n",
    "                   'met_dataset':infile,\n",
    "                   'aq_input_data':infile,\n",
    "                   'o3_dataset': infile,\n",
    "                   'monitor_points':monitor_points\n",
    "                 }\n",
    "    return training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limiting_points(cell_list,grid_sample_limit):\n",
    "    return cell_list[np.logical_or(np.logical_and((cell_list<grid_sample_limit[0][1]),\n",
    "    (cell_list>grid_sample_limit[0][0])),np.logical_and((cell_list<grid_sample_limit[1][1]),\n",
    "    (cell_list>grid_sample_limit[1][0])),np.logical_and((cell_list<grid_sample_limit[2][1]),\n",
    "                                                        (cell_list>grid_sample_limit[2][0])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_function(all_points,x_no,y_no):\n",
    "    x_cells=[]\n",
    "    y_cells=[]\n",
    "    \n",
    "    x_cells=random.sample(list(all_grids_x),k=x_no)\n",
    "    y_cells=random.sample(list(all_grids_y),k=y_no)  \n",
    "    x_limited=limiting_points(np.array(x_cells),grid_sample_limit_x)\n",
    "    y_limited=limiting_points(np.array(y_cells),grid_sample_limit_y)\n",
    "\n",
    "    x_y=list(set(product(x_limited,y_limited)))\n",
    "    return x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tqdm_function(x_y_t_f,files,a,look_back,f_h):\n",
    "      predictors={}\n",
    "      aq_data={}\n",
    "      aq_input={}\n",
    "      def round_func(val):\n",
    "        if(val%1>0):\n",
    "          return int(val)+1\n",
    "        else:\n",
    "          return int(val)\n",
    "      l_edge=files[round_func(look_back/24) -1]\n",
    "      r_edge=files[-1*int(f_h/24)]\n",
    "      lb_ndarr,fh_ndarr=hours_list(a,look_back,f_h)\n",
    "      for i in x_y_t_f:\n",
    "          if((i[0]>l_edge) and (i[0]<r_edge)):\n",
    "              #inp_file_list = range(i[0]-round_func(look_back/24),i[0]+1)\n",
    "              #out_file_list=range(i[0],i[0]+int(f_h/24)+1)\n",
    "              #o3_dataset=train_param(out_file_list)['o3_dataset']\n",
    "\n",
    "\n",
    "              point=i[1][0]#[0]                 ## x y coordinates of the current point\n",
    "\n",
    "\n",
    "              #hour_next = a[i[1][1]]+1  #hour to be forecasted.\n",
    "              point_x=get_index(all_grids_x,int(point[0]))\n",
    "              point_y=get_index(all_grids_y,int(point[1]))\n",
    "              #lb_ndarr,fh_ndarr=hours_list(look_back,f_h)\n",
    "              #print(list(inp_file_list))\n",
    "              #print(list(out_file_list))\n",
    "              #o3_dataset=o3_dataset['O3']\n",
    "              hours_input=list_seg(i[1][1],look_back,lb_ndarr)\n",
    "              hours_output=list_seg_future(i[1][1],f_h,fh_ndarr)\n",
    "              #print(type(o3_dataset[fh_ndarr[i[1][1]],point_x,point_y]))\n",
    "              inp_file_list=range(i[0]-len(hours_input),i[0])#+1)\n",
    "              out_file_list=range(i[0],i[0]+len(hours_output))\n",
    "              aq_data[i]=get_future_data(out_file_list,hours_output,point_x,point_y)\n",
    "              print(len(aq_data[i]))\n",
    "              #print(aq_data[i])\n",
    "              aq_input[i]= get_aq_input(point[0],point[1],hours_input,inp_file_list)\n",
    "              print(i[0])\n",
    "              #print(type(get_predictors_xyt(point,lb_ndarr[i[1][1]],training_params,debug=False)))\n",
    "              predictors[i]=get_predictors_xyt(point,hours_input,inp_file_list,debug=False)\n",
    "              #print(len(predictors[i][1]))\n",
    "              #print(predictors[i][1])\n",
    "              #print(aq_data[i])\n",
    "      return predictors, aq_data, aq_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_ret(nfiles,a,n1,n2,look_back,f_h):\n",
    "    aq_data={}\n",
    "\n",
    "    predictors={}\n",
    "    aq_input={}\n",
    "    x_y=[]\n",
    "    x_y=sampling_function(all_points,n1,n2)\n",
    "    x_y.append(tuple([58,121]))\n",
    "    x_y_t=product(x_y,b)\n",
    "    x_y_t=list(x_y_t)\n",
    "    \n",
    "    for f_no in range(len(nfiles)):\n",
    "        x_y_t_f=product(nfiles[f_no],x_y_t) \n",
    "\n",
    "        x_y_t_f=list(x_y_t_f)\n",
    "        print(len(x_y_t_f))\n",
    "        print(f_no)\n",
    "        predictors_temp,aq_data_temp,aq_input_temp=tqdm_function(x_y_t_f,nfiles[f_no],a,look_back,f_h)\n",
    "        predictors.update(predictors_temp)\n",
    "        aq_data.update(aq_data_temp)\n",
    "        aq_input.update(aq_input_temp)\n",
    "\n",
    "    return predictors,aq_data,aq_input,x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24450\n",
      "0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/woshi/Desktop/CE_675_Project/Data/short_data/182.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-dab05bd3c772>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mf_h\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#a=[2,4,5,7,9,13,15,17,19,22] #the same list of hours is repeated for all the days\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpredictors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeat_ret\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnfiles_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# print ('\\n Done getting predictors')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# In[ ]:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-2dec07a7f64b>\u001b[0m in \u001b[0;36mfeat_ret\u001b[1;34m(nfiles, a, n1, n2, look_back, f_h)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_y_t_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mpredictors_temp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_data_temp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_input_temp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtqdm_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_y_t_f\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf_no\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mpredictors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictors_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0maq_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maq_data_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-798667aa1bc5>\u001b[0m in \u001b[0;36mtqdm_function\u001b[1;34m(x_y_t_f, files, a, look_back, f_h)\u001b[0m\n\u001b[0;32m     33\u001b[0m               \u001b[0minp_file_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhours_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#+1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m               \u001b[0mout_file_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhours_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m               \u001b[0maq_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_future_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_file_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhours_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m               \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maq_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m               \u001b[1;31m#print(aq_data[i])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-32e2a019de18>\u001b[0m in \u001b[0;36mget_future_data\u001b[1;34m(o_f_l, tlist, point_x, point_y)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_future_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo_f_l\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtlist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo_f_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m       \u001b[0maq_output_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo_f_l\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'o3_dataset'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m       \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0maq_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maq_output_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'O3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_y\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-70ae3cb6d99b>\u001b[0m in \u001b[0;36mtrain_param\u001b[1;34m(file_num)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0minfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshort_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.npz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m#print(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/woshi/Desktop/CE_675_Project/Data/short_data/182.npz'"
     ]
    }
   ],
   "source": [
    "look_back=30\n",
    "f_h=24\n",
    "#a=[2,4,5,7,9,13,15,17,19,22] #the same list of hours is repeated for all the days\n",
    "predictors,aq_data,aq_input,x_y=feat_ret(nfiles_train,a,25,15,look_back,f_h)\n",
    "# print ('\\n Done getting predictors')\n",
    "# In[ ]:\n",
    "\n",
    "predictors_future,aq_data_future,aq_input_future,x_y_future=feat_ret(nfiles_train,a,15,12,look_back,f_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting train and test splits\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predictors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-29f9001735c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mread_from_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Getting train and test splits'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mpredictors_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maq_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictors_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maq_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mpredictors_train_future\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_train_future\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictors_test_future\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictors_future\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_data_future\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictors' is not defined"
     ]
    }
   ],
   "source": [
    "save_to_file = True\n",
    "read_from_file = False\n",
    "analyze = True\n",
    "create_plots = True\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "\n",
    "def get_splits(predictors,aq_data,train_fraction=0.95):\n",
    "    num_predictors = len(predictors)\n",
    "    pxy = list(predictors.keys())\n",
    "    #print(len(aq_data))\n",
    "    num_train = int(train_fraction*num_predictors)\n",
    "    train_indices = np.random.choice(np.arange(num_predictors),num_train, replace=False)\n",
    "    pxy_train = [pxy[i] for i in train_indices]\n",
    "    predictors_train = [predictors[i] for i in pxy_train]\n",
    "    aq_train=[]\n",
    "    for i in pxy_train:\n",
    "        aq_train.append(aq_data[i])\n",
    "    #aq_train = [aq_data[i] for i in pxy_train]\n",
    "    print ('number of training points', num_train, '&', len(predictors_train))\n",
    "\n",
    "    #Random sample for Testing\n",
    "    predictors_test = [predictors[i] for i in pxy if i not in pxy_train]\n",
    "    aq_test = [aq_data[i] for i in pxy if i not in pxy_train]\n",
    "    pxy_test = [i for i in pxy if i not in pxy_train]\n",
    "    print('number of testing points', len(predictors_test), '&', len(aq_test))\n",
    "    return predictors_train, aq_train, predictors_test, aq_test\n",
    "\n",
    "if not read_from_file:\n",
    "    print('Getting train and test splits')\n",
    "    predictors_train, aq_train, predictors_test, aq_test = get_splits(predictors,aq_data)\n",
    "    predictors_train_future,aq_train_future,predictors_test_future = get_splits(predictors_future,aq_data_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictors_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-4cecfcbf7532>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictors_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictors_train_future\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictors_train' is not defined"
     ]
    }
   ],
   "source": [
    "# ### 6. Data Standerdization\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "type(predictors_train)\n",
    "type(predictors_train_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[16]:\n",
    "\n",
    "\n",
    "def scaling_data(predictors,aq_data):\n",
    "    if not read_from_file:\n",
    "    \n",
    "        print('scaling inputs and outputs')\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "        data = np.concatenate(list(predictors.values()))\n",
    "        print(len(data[1]))\n",
    "        pred_scaler = MinMaxScaler()\n",
    "        pred_scaler.fit(data)\n",
    "    \n",
    "        print(len(aq_data))\n",
    "        for i in aq_data:\n",
    "          print(len(aq_data[i]))\n",
    "        aq_data_1 = np.stack(aq_data.values()).reshape(-1,24)\n",
    "        aq_scaler = MinMaxScaler()\n",
    "        aq_scaler.fit(aq_data_1)\n",
    "    \n",
    "        predictors_train_scaled = [pred_scaler.transform(mx) for mx in predictors_train]\n",
    "        predictors_train_rnn = np.stack(predictors_train_scaled)\n",
    "        aq_train_rnn = aq_scaler.transform(np.stack(aq_train).reshape(-1,24))\n",
    "    \n",
    "        predictors_test_scaled = [pred_scaler.transform(mx) for mx in predictors_test]\n",
    "        predictors_test_rnn = np.stack(predictors_test_scaled)\n",
    "        aq_test_rnn = aq_scaler.transform(np.stack(aq_test).reshape(-1,24))\n",
    "    return predictors_train_rnn,predictors_test_rnn,aq_train_rnn,aq_test_rnn,pred_scaler,aq_scaler\n",
    "output_dir='/mnt/raid2/System/home/mmohan3/weather_data/output_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-8d0b6917bfd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msave_to_file\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#data_to_save = [predictors_train_rnn, aq_train, predictors_test_rnn,aq_test]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mpredictors_train_rnn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictors_test_rnn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_train_rnn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_test_rnn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred_scaler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_scaler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaling_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     np.savez(predictors_file, predictors_train_rnn = predictors_train_rnn, #X_train, shape=(samples,tsteps,features)\n\u001b[0;32m      8\u001b[0m                               \u001b[0maq_train_rnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maq_train_rnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#y_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictors' is not defined"
     ]
    }
   ],
   "source": [
    "### Save for future use.\n",
    "predictors_file = '/mnt/raid2/System/home/mmohan3/predictors_data3_poster.npz'\n",
    "predictors_future_file = '/mnt/raid2/System/home/mmohan3/predictors_data3_future.npz'\n",
    "if save_to_file == True:\n",
    "    #data_to_save = [predictors_train_rnn, aq_train, predictors_test_rnn,aq_test] \n",
    "    predictors_train_rnn,predictors_test_rnn,aq_train_rnn,aq_test_rnn,pred_scaler,aq_scaler=scaling_data(predictors,aq_data)\n",
    "    np.savez(predictors_file, predictors_train_rnn = predictors_train_rnn, #X_train, shape=(samples,tsteps,features)\n",
    "                              aq_train_rnn = aq_train_rnn, #y_train\n",
    "                              predictors_test_rnn = predictors_test_rnn, #X_test\n",
    "                              aq_test_rnn = aq_test_rnn) #y_test)\n",
    "    dump(pred_scaler,output_dir+'pred_scaler.joblib')\n",
    "    dump(aq_scaler,output_dir+'aq_scaler.joblib')\n",
    "    predictors_train_future_rnn,predictors_test_future_rnn,aq_train_future_rnn,aq_test_future_rnn,pred_future_scaler,aq_future_scaler=scaling_data(predictors_future,aq_data)\n",
    "    np.savez(predictors_future_file, predictors_train_future_rnn = predictors_train_future_rnn, #X_train, shape=(samples,tsteps,features)\n",
    "                              aq_train_future_rnn = aq_train_future_rnn, #y_train\n",
    "                              predictors_test_future_rnn = predictors_test_future_rnn, #X_test\n",
    "                              aq_test_future_rnn = aq_test_future_rnn) #y_test        \n",
    "    dump(pred_scaler,output_dir+'pred_future_scaler.joblib')\n",
    "    dump(aq_scaler,output_dir+'aq_future_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
