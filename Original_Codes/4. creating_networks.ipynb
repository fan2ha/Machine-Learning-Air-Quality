{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import netCDF4 as ncf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[2,4,7,9,13,17,19]        #a is the hour for which we're trying to predict\n",
    "b=[0,1,2,3,4,5,6]      # b is indexing for a\n",
    "\n",
    "#### dates generator. 6 weeks.\n",
    "no_files=5\n",
    "nfiles_train=[]\n",
    "nfiles_test=[]\n",
    "day_count=5\n",
    "no_of_months=1\n",
    "for i in range(no_of_months):\n",
    "    nfiles_temp=list(range(day_count,day_count+15))  #7\n",
    "    nfiles_train.append(nfiles_temp)\n",
    "    nfiles_temp=list(range(day_count+16,day_count+22))  #11\n",
    "    nfiles_test.append(nfiles_temp)\n",
    "    day_count=day_count+30\n",
    "# find days of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Grid reference both x-axis,y-axis\n",
    "loc_y = [165,121,171]\n",
    "loc_x = [67,58,39]\n",
    "\n",
    "grid_size_x = dx = 20\n",
    "grid_size_y = dy = 20\n",
    "\n",
    "grid_sample_limit_x=[]\n",
    "grid_sample_limit_y=[]\n",
    "for i in loc_x:\n",
    "    grid_sample_limit_x.append((i-15,i+15))\n",
    "for i in loc_y:\n",
    "    grid_sample_limit_y.append((i-15,i+15))\n",
    "\n",
    "grids_x = [np.arange(x-dx, x+dx, 1) for x in loc_x]\n",
    "grids_y = [np.arange(y-dy, y+dy, 1) for y in loc_y]\n",
    "\n",
    "all_grids_x = np.concatenate(grids_x)\n",
    "all_grids_y = np.concatenate(grids_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitors_file = 'C:/Users/woshi/Desktop/CE_675_Project/Data/loc_data/monitor_list_NC.csv'\n",
    "with open(monitors_file) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    monitor_points = [(int(float(row['Col'])), int(float(row['Row'] ))) for row in reader if row['Col']] \n",
    "\n",
    "monitors_x, monitors_y = zip(*monitor_points)\n",
    "\n",
    "all_grids_x=np.concatenate([all_grids_x,monitors_x])\n",
    "all_grids_y=np.concatenate([all_grids_y,monitors_y])\n",
    "\n",
    "all_points = list(product(all_grids_x, all_grids_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the entire file list in a directory\n",
    "def get_file_list(data_dir):\n",
    "    return [os.path.join(data_dir, name) for name in os.listdir(data_dir)]\n",
    "#function to get index for a point based on grid reference.\n",
    "def get_index(grid,val):\n",
    "    try:\n",
    "        return np.where(grid==val)[0][0]\n",
    "    except (TypeError,IndexError):\n",
    "            if(len(val)>1):\n",
    "                index_list=[]\n",
    "                for i in val:\n",
    "                    index_list.append(np.where(grid==i)[0][0])\n",
    "                return index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_dir='C:/Users/woshi/Desktop/CE_675_Project/Data/short_data/'\n",
    "\n",
    "short_files = get_file_list(short_dir)\n",
    "#short_files=short_files[:30]\n",
    "#it's the list of list of all indices for the hours to be predicted everyday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 10 time points\n",
    "\n",
    "def list_seg(n,look_back,lb_ndarr):\n",
    "    t_part_list=[]\n",
    "    i=0\n",
    "    tlist=lb_ndarr[n]\n",
    "    def creating_tlist(tlist,j,ind):\n",
    "        try:\n",
    "            ind=tlist.index(0-j*24)\n",
    "            t_part_list.append([i+(0+j*24) for i in tlist[ind:]])\n",
    "            tlist=tlist[:ind]\n",
    "            j=j+1\n",
    "            return creating_tlist(tlist,j,ind)\n",
    "        except ValueError:\n",
    "            t_part_list.append([i+(0+j*24) for i in tlist[:ind]])\n",
    "            return t_part_list\n",
    "    t_part_list=creating_tlist(tlist,i,0)\n",
    "    return t_part_list #returns the required sets of indices for a particular hour at request\n",
    "\n",
    "def hours_list(a,l_b,f_h):\n",
    "    lb_ndarr = []\n",
    "    for i in a:\n",
    "        tlist=list(range(i-l_b,i))           # this is where first_ndarr is created\n",
    "        lb_ndarr.append(tlist)\n",
    "    fh_ndarr=[] #this is the list of all indices for the future data collection\n",
    "    for i in a:\n",
    "        tlist=list(range(i+1,i+f_h+1))\n",
    "        fh_ndarr.append(tlist)\n",
    "    return lb_ndarr,fh_ndarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_seg_future(n,f_h,fh_ndarr):    #if n=2; a[2]=4; tlist=5:29; firstpart=5:23;lastpart=0:5\n",
    "    t_part_list=[]\n",
    "    i=1\n",
    "    tlist=fh_ndarr[n]\n",
    "    def creating_tlist(tlist,j,ind):\n",
    "            try:\n",
    "                ind=tlist.index(0+j*24)\n",
    "                t_part_list.append([i-(0+j*24) for i in tlist[:ind]])\n",
    "                tlist=tlist[ind:]\n",
    "                \n",
    "                j=j+1\n",
    "                return creating_tlist(tlist,j,ind)\n",
    "            except ValueError:\n",
    "                t_part_list.append([i-(0+j*24) for i in tlist[:24-ind]])\n",
    "                return t_part_list\n",
    "    t_part_list=creating_tlist(tlist,i,0)\n",
    "    return t_part_list\n",
    "\n",
    "# maybe has the same functionality as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_predictors(x,y, monitor_points):\n",
    "    \"\"\"\n",
    "    Returns distance and angle for an arbitrary point in the domain\n",
    "    w.r.t. monitor network. These are w.r.t. each monitor or centroid of all monitors\n",
    "    \n",
    "    Args:\n",
    "        x: x_cooridate of arbitrary point\n",
    "        y: y_coordinate of arbitrary point\n",
    "        training_params: Training parameters containing locations of monitors\n",
    "        centroid: if True, returns distance and angle from the centrod of the monitors\n",
    "    \n",
    "    Returns:\n",
    "        Angles and distances \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    centroid=True\n",
    "    x_centroid = np.average([point[0] for point in monitor_points])\n",
    "    y_centroid = np.average([point[1] for point in monitor_points])\n",
    "   \n",
    "    def unit_vector(vector):\n",
    "        \"\"\" Returns the unit vector of the vector.\"\"\"\n",
    "        return vector / np.linalg.norm(vector)\n",
    "\n",
    "    def angle_between(v1, v2):\n",
    "        \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "                >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "                1.5707963267948966\n",
    "                >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "                0.0\n",
    "                >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "                3.141592653589793\n",
    "        \"\"\"\n",
    "        v1_u = unit_vector(v1)\n",
    "        v2_u = unit_vector(v2)\n",
    "        return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "    if centroid:   \n",
    "        distance = math.sqrt((x_centroid - x)**2 + (y_centroid -y)**2)\n",
    "        v1 = (x_centroid, y_centroid)\n",
    "        v2 = (x,y)\n",
    "        angle = angle_between(v1,v2)\n",
    "        return np.array([distance, angle])\n",
    "    else:\n",
    "        \n",
    "        distances =[math.sqrt((x - point[0])**2 + (y - point[1])**2) for point in monitor_points]\n",
    "        angles = [angle_between((x,y), (point[0], point[1])) for point in monitor_points]\n",
    "        da = distances + angles\n",
    "        return np.array(da) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit(nopython=True)\n",
    "def emis_predictors(x,y,i_f_l,tlist):\n",
    "    d=5\n",
    "    xs = np.arange(-d,d+1) + x\n",
    "    ys = np.arange(-d,d+1) + y\n",
    "\n",
    "    num_cells = len(xs)*len(ys)\n",
    "    xs_indexed=get_index(all_grids_x,xs)\n",
    "    ys_indexed=get_index(all_grids_y,ys)\n",
    "    \n",
    "    def get_emis_slice(emis_dataset,species,tlist):\n",
    "            grid = np.ix_(tlist,xs_indexed,ys_indexed)\n",
    "            e1 = emis_dataset[species][:]\n",
    "            e2 = e1[grid]\n",
    "            return e2.reshape(len(tlist), num_cells)\n",
    "    for i in range(len(i_f_l)):\n",
    "      emis_dataset=train_param(i_f_l[i])['emis_dataset']\n",
    "      emis_species=train_param(i_f_l[i])['emis_species']\n",
    "      if(i==0):\n",
    "        emis_seq = [ get_emis_slice(emis_dataset, species,tlist[i]) for species in emis_species] \n",
    "        emis_seq = np.concatenate(emis_seq,axis=1)\n",
    "        emis_seq_total=emis_seq\n",
    "      else:\n",
    "          emis_seq = [ get_emis_slice(emis_dataset, species,tlist[i]) for species in emis_species] \n",
    "          emis_seq = np.concatenate(emis_seq,axis=1)\n",
    "          emis_seq_total=np.vstack((emis_seq_total,emis_seq))\n",
    "    return emis_seq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_monitors_predictors(t,i_f_l, monitor_points):\n",
    "\n",
    "    monitors_x, monitors_y = zip(*monitor_points) \n",
    "\n",
    "    monitors_x_edit=get_index(all_grids_x,monitors_x)\n",
    "    monitors_y_edit=get_index(all_grids_y,monitors_y)\n",
    "\n",
    "    for i in range(len(i_f_l)):\n",
    "        aq_input_data=train_param(i_f_l[i])['aq_input_data']  \n",
    "        if(i==0):\n",
    "          concs = aq_input_data['O3'][t[i]][:,monitors_x_edit,monitors_y_edit]\n",
    "          concs_total=concs\n",
    "        else:\n",
    "          concs = aq_input_data['O3'][t[i]][:,monitors_x_edit,monitors_y_edit]\n",
    "          concs_total=np.vstack((concs_total,concs))  \n",
    "    return concs_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def met_predictors(t,i_f_l,monitor_points):\n",
    "   \n",
    "    monitors_x, monitors_y = zip(*monitor_points)     \n",
    "    monitors_x_edit=get_index(all_grids_x,monitors_x)    #function to get index\n",
    "    monitors_y_edit=get_index(all_grids_y,monitors_y)    #function to get index\n",
    "    \n",
    "    def get_met(met_dataset,met_params,t):\n",
    "      met_seq = []\n",
    "      for param in met_params:\n",
    "          m1 = met_dataset[param][:]\n",
    "          m2 = m1[t][:,monitors_x_edit,monitors_y_edit]\n",
    "          met_seq.append(m2)\n",
    "      return met_seq\n",
    "    for i in range(len(i_f_l)):\n",
    "        met_dataset=train_param(i_f_l[i])['met_dataset']\n",
    "        met_params=train_param(i_f_l[i])['met_params']\n",
    "        if(i==0):\n",
    "            met_seq = get_met(met_dataset,met_params,t[i])\n",
    "            met_seq=np.concatenate(met_seq, axis=1)\n",
    "            met_seq_total=met_seq\n",
    "        else:\n",
    "            met_seq = get_met(met_dataset,met_params,t[i])\n",
    "            met_seq=np.concatenate(met_seq, axis=1)\n",
    "            met_seq_total=np.vstack((met_seq_total,met_seq))\n",
    "    return met_seq_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictors_xyt(p,tlist,i_f_l,debug=False):\n",
    "    \"\"\"\n",
    "    For each point and timestamp(s) generate a row of predictors\n",
    "    \"\"\"\n",
    "    \n",
    "    x=p[0]\n",
    "    y=p[1]\n",
    "    dist = dist_predictors(x,y,monitor_points) #time invariant\n",
    "    dist_tile = np.tile(dist,(sum([len(i) for i in tlist]),1))\n",
    "\n",
    "    \n",
    "    emis = emis_predictors(x,y,i_f_l,tlist)\n",
    "    met  = met_predictors(tlist,i_f_l,monitor_points)\n",
    "    \n",
    "    hist = history_monitors_predictors(tlist,i_f_l,monitor_points)\n",
    "   \n",
    "    try:\n",
    "        preds = np.concatenate([dist_tile, emis, hist,met], axis=1)\n",
    "        return preds\n",
    "    except:\n",
    "        return [dist_tile,emis,hist,met]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aq_input(x,y,t,i_f_l):\n",
    "    x_indexed=get_index(all_grids_x,x)\n",
    "    y_indexed=get_index(all_grids_y,y)\n",
    "    \n",
    "    for i in range(len(i_f_l)):\n",
    "        #print(len(i_f_l))\n",
    "        #print(len(t))\n",
    "        \n",
    "        aq_input_data=train_param(i_f_l[i])['aq_input_data']  \n",
    "        if(i==0):\n",
    "          aq_input = aq_input_data['O3'][t[i]][:,x_indexed,y_indexed]\n",
    "          aq_input_total=aq_input\n",
    "        else:\n",
    "          aq_input = aq_input_data['O3'][t[i]][:,x_indexed,y_indexed]\n",
    "          aq_input_total=np.concatenate((aq_input_total,aq_input))  \n",
    "    return aq_input_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_future_data(o_f_l,tlist,point_x,point_y):\n",
    "    for i in range(len(o_f_l)):\n",
    "      aq_output_data=train_param(o_f_l[i])['o3_dataset']\n",
    "      if(i==0):\n",
    "        aq_output=aq_output_data['O3'][tlist[i],point_x,point_y]\n",
    "        aq_output_total=aq_output\n",
    "        #print(len(aq_output))\n",
    "      else:\n",
    "        aq_output=aq_output_data['O3'][tlist[i],point_x,point_y]\n",
    "        print(aq_output_total)\n",
    "        print(aq_output)\n",
    "        aq_output_total=np.concatenate((aq_output_total,aq_output))\n",
    "    \n",
    "    return aq_output_total    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_param(file_num):\n",
    "    infile=np.load(short_dir+str(file_num)+'.npz')\n",
    "    \n",
    "\n",
    "        #print(i)\n",
    "    \n",
    "    #outfile=np.load(short_dir+str(out_file_list[0])+'.npz')\n",
    "    #outfile=outfile['O3']\n",
    "    #f_output=tempfile.TemporaryFile()\n",
    "    #for i in range(1,len(out_file_list)):\n",
    "    #    file_1=np.load(short_dir+str(out_file_list[i])+'.npz')\n",
    "    #    np.savez(f_output,O3=create_npz(file_1,outfile,'O3'))\n",
    "    #    _ = f_output.seek(0) # Only needed here to simulate closing & reopening file\n",
    "    #    outfile=np.load(f_output)\n",
    "    #f_output = tempfile.NamedTemporaryFile(delete=True)\n",
    "    #f_output.close()\n",
    "    training_params = {\n",
    "                   'd' : 5, #local_emissions_size\n",
    "                   'emis_species': ['NO', 'NO2'], #emission_species\n",
    "                   'met_params':['PBL', 'Q2', 'TEMP2', 'WSPD10', 'WDIR10'], #met parameters\n",
    "                   #'met_params': ['PBL', 'WDIR10'],\n",
    "                   'emis_dataset': infile,\n",
    "                   'met_dataset':infile,\n",
    "                   'aq_input_data':infile,\n",
    "                   'o3_dataset': infile,\n",
    "                   'monitor_points':monitor_points\n",
    "                 }\n",
    "    return training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[11]:  #This function is to\n",
    "def limiting_points(cell_list,grid_sample_limit):\n",
    "    return cell_list[np.logical_or(np.logical_and((cell_list<grid_sample_limit[0][1]),(cell_list>grid_sample_limit[0][0])),\n",
    "                                   np.logical_and((cell_list<grid_sample_limit[1][1]),(cell_list>grid_sample_limit[1][0])),\n",
    "                                   np.logical_and((cell_list<grid_sample_limit[2][1]),(cell_list>grid_sample_limit[2][0])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_function(all_points,x_no,y_no):\n",
    "    x_cells=[]\n",
    "    y_cells=[]\n",
    "    \n",
    "    x_cells=random.sample(list(all_grids_x),k=x_no)\n",
    "    y_cells=random.sample(list(all_grids_y),k=y_no)  \n",
    "    x_limited=limiting_points(np.array(x_cells),grid_sample_limit_x)\n",
    "    y_limited=limiting_points(np.array(y_cells),grid_sample_limit_y)\n",
    "\n",
    "    x_y=list(set(product(x_limited,y_limited)))\n",
    "    return x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tqdm_function(x_y_t_f,files,a,look_back,f_h):\n",
    "      predictors={}\n",
    "      aq_data={}\n",
    "      aq_input={}\n",
    "      def round_func(val):\n",
    "        if(val%1>0):\n",
    "          return int(val)+1\n",
    "        else:\n",
    "          return int(val)\n",
    "      l_edge=files[round_func(look_back/24) -1]\n",
    "      r_edge=files[-1*int(f_h/24)]\n",
    "      lb_ndarr,fh_ndarr=hours_list(a,look_back,f_h)\n",
    "      for i in x_y_t_f:\n",
    "          if((i[0]>l_edge) and (i[0]<r_edge)):\n",
    "              #inp_file_list = range(i[0]-round_func(look_back/24),i[0]+1)\n",
    "              #out_file_list=range(i[0],i[0]+int(f_h/24)+1)\n",
    "              #o3_dataset=train_param(out_file_list)['o3_dataset']\n",
    "\n",
    "\n",
    "              point=i[1][0]#[0]                 ## x y coordinates of the current point\n",
    "\n",
    "\n",
    "              #hour_next = a[i[1][1]]+1  #hour to be forecasted.\n",
    "              point_x=get_index(all_grids_x,int(point[0]))\n",
    "              point_y=get_index(all_grids_y,int(point[1]))\n",
    "              #lb_ndarr,fh_ndarr=hours_list(look_back,f_h)\n",
    "              #print(list(inp_file_list))\n",
    "              #print(list(out_file_list))\n",
    "              #o3_dataset=o3_dataset['O3']\n",
    "              hours_input=list_seg(i[1][1],look_back,lb_ndarr)\n",
    "              hours_output=list_seg_future(i[1][1],f_h,fh_ndarr)\n",
    "              #print(type(o3_dataset[fh_ndarr[i[1][1]],point_x,point_y]))\n",
    "              inp_file_list=range(i[0]-len(hours_input),i[0])#+1)\n",
    "              out_file_list=range(i[0],i[0]+len(hours_output))\n",
    "              aq_data[i]=get_future_data(out_file_list,hours_output,point_x,point_y)\n",
    "              print(len(aq_data[i]))\n",
    "              #print(aq_data[i])\n",
    "              aq_input[i]= get_aq_input(point[0],point[1],hours_input,inp_file_list)\n",
    "              print(i[0])\n",
    "              #print(type(get_predictors_xyt(point,lb_ndarr[i[1][1]],training_params,debug=False)))\n",
    "              predictors[i]=get_predictors_xyt(point,hours_input,inp_file_list,debug=False)\n",
    "              #print(len(predictors[i][1]))\n",
    "              #print(predictors[i][1])\n",
    "              #print(aq_data[i])\n",
    "      return predictors, aq_data, aq_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_ret(nfiles,a,n1,n2,look_back,f_h):\n",
    "    aq_data={}\n",
    "\n",
    "    predictors={}\n",
    "    aq_input={}\n",
    "    x_y=[]\n",
    "    x_y=sampling_function(all_points,n1,n2)\n",
    "    x_y.append(tuple([58,121]))\n",
    "    x_y_t=product(x_y,b)\n",
    "    x_y_t=list(x_y_t)\n",
    "    \n",
    "    for f_no in range(len(nfiles)):\n",
    "        x_y_t_f=product(nfiles[f_no],x_y_t) \n",
    "\n",
    "        x_y_t_f=list(x_y_t_f)\n",
    "        print(len(x_y_t_f))\n",
    "        print(f_no)\n",
    "        predictors_temp,aq_data_temp,aq_input_temp=tqdm_function(x_y_t_f,nfiles[f_no],a,look_back,f_h)\n",
    "        predictors.update(predictors_temp)\n",
    "        aq_data.update(aq_data_temp)\n",
    "        aq_input.update(aq_input_temp)\n",
    "\n",
    "    return predictors,aq_data,aq_input,x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9345\n",
      "0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/woshi/Desktop/CE_675_Project/Data/short_data/7.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-9c93b0fff439>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlook_back\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mf_h\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpredictors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeat_ret\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnfiles_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'\\n Done getting predictors'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-2dec07a7f64b>\u001b[0m in \u001b[0;36mfeat_ret\u001b[1;34m(nfiles, a, n1, n2, look_back, f_h)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_y_t_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mpredictors_temp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_data_temp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maq_input_temp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtqdm_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_y_t_f\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf_no\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mpredictors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictors_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0maq_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maq_data_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-798667aa1bc5>\u001b[0m in \u001b[0;36mtqdm_function\u001b[1;34m(x_y_t_f, files, a, look_back, f_h)\u001b[0m\n\u001b[0;32m     33\u001b[0m               \u001b[0minp_file_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhours_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#+1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m               \u001b[0mout_file_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhours_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m               \u001b[0maq_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_future_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_file_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhours_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m               \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maq_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m               \u001b[1;31m#print(aq_data[i])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-4044f16565c1>\u001b[0m in \u001b[0;36mget_future_data\u001b[1;34m(o_f_l, tlist, point_x, point_y)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_future_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo_f_l\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtlist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo_f_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m       \u001b[0maq_output_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo_f_l\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'o3_dataset'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m       \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0maq_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maq_output_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'O3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_y\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-70ae3cb6d99b>\u001b[0m in \u001b[0;36mtrain_param\u001b[1;34m(file_num)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0minfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshort_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.npz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m#print(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/woshi/Desktop/CE_675_Project/Data/short_data/7.npz'"
     ]
    }
   ],
   "source": [
    "##Commented after data extraction\n",
    "\n",
    "look_back=30\n",
    "f_h=24\n",
    "predictors,aq_data,aq_input,x_y=feat_ret(nfiles_train,a,25,15,look_back,f_h)\n",
    "print ('\\n Done getting predictors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Create Random Split for Training and Testing\n",
    "save_to_file = True\n",
    "read_from_file = True\n",
    "analyze = True\n",
    "create_plots = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(predictors,train_fraction=0.95):\n",
    "    num_predictors = len(predictors)\n",
    "    pxy = list(predictors.keys())\n",
    "    #print(len(aq_data))\n",
    "    num_train = int(train_fraction*num_predictors)\n",
    "    train_indices = np.random.choice(np.arange(num_predictors),num_train, replace=False)\n",
    "    pxy_train = [pxy[i] for i in train_indices]\n",
    "    predictors_train = [predictors[i] for i in pxy_train]\n",
    "    aq_train=[]\n",
    "    for i in pxy_train:\n",
    "        aq_train.append(aq_data[i])\n",
    "    #aq_train = [aq_data[i] for i in pxy_train]\n",
    "    print ('number of training points', num_train, '&', len(predictors_train))\n",
    "\n",
    "    #Random sample for Testing\n",
    "    predictors_test = [predictors[i] for i in pxy if i not in pxy_train]\n",
    "    aq_test = [aq_data[i] for i in pxy if i not in pxy_train]\n",
    "    pxy_test = [i for i in pxy if i not in pxy_train]\n",
    "    print('number of testing points', len(predictors_test), '&', len(aq_test))\n",
    "    return predictors_train, aq_train, predictors_test, aq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Commented after data extraction\n",
    "\n",
    "\n",
    "if not read_from_file:\n",
    "   print('Getting train and test splits')\n",
    "   predictors_train, aq_train, predictors_test, aq_test = get_splits(predictors)\n",
    "### 6. Data Standerdization\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "type(predictors_train)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "# =============================================================================\n",
    "# \n",
    "if not read_from_file:\n",
    "\n",
    "    print('scaling inputs and outputs')\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    data = np.concatenate(list(predictors.values()))\n",
    "    print(len(data[1]))\n",
    "    pred_scaler = MinMaxScaler()\n",
    "    pred_scaler.fit(data)\n",
    "\n",
    "    print(len(aq_data))\n",
    "    for i in aq_data:\n",
    "      print(len(aq_data[i]))\n",
    "    aq_data_1 = np.stack(aq_data.values()).reshape(-1,24)\n",
    "    aq_scaler = MinMaxScaler()\n",
    "    aq_scaler.fit(aq_data_1)\n",
    "\n",
    "    predictors_train_scaled = [pred_scaler.transform(mx) for mx in predictors_train]\n",
    "    predictors_train_rnn = np.stack(predictors_train_scaled)\n",
    "    aq_train_rnn = aq_scaler.transform(np.stack(aq_train).reshape(-1,24))\n",
    "\n",
    "    predictors_test_scaled = [pred_scaler.transform(mx) for mx in predictors_test]\n",
    "    predictors_test_rnn = np.stack(predictors_test_scaled)\n",
    "    aq_test_rnn = aq_scaler.transform(np.stack(aq_test).reshape(-1,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "### Save for future use.\n",
    "predictors_file = 'C:/Users/woshi/Desktop/CE_675_Project/Data/predictors/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "if save_to_file == True:\n",
    "\n",
    "    \n",
    "    data_to_save = [predictors_train_rnn, aq_train, predictors_test_rnn,aq_test] \n",
    "\n",
    "    np.savez(predictors_file, predictors_train_rnn = predictors_train_rnn, #X_train, shape=(samples,tsteps,features)\n",
    "                              aq_train_rnn = aq_train_rnn, #y_train\n",
    "                              predictors_test_rnn = predictors_test_rnn, #X_test\n",
    "                              aq_test_rnn = aq_test_rnn) #y_test\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_arrays=np.load(predictors_file)\n",
    "# In[17]:\n",
    "predictors_train_rnn = predictors_arrays['predictors_train_rnn']\n",
    "aq_train_rnn=predictors_arrays['aq_train_rnn']\n",
    "predictors_test_rnn = predictors_arrays['predictors_test_rnn']\n",
    "aq_test_rnn=predictors_arrays['aq_test_rnn']\n",
    "#for i in range(len(predictors_train_scaled)):\n",
    "#   print(len(predictors_train_scaled[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "predictors_future,aq_data_future,aq_input,x_y=feat_ret(nfiles_test,a,15,15,look_back,f_h)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM\n",
    "from keras.utils import np_utils\n",
    "import copy\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0,s1,s2=predictors_train_rnn.shape\n",
    "print ('shape of x_train data', s0,s1,s2)\n",
    "print ('shape of y_train data', aq_train_rnn.shape)\n",
    "output_dir='/mnt/raid2/System/home/mmohan3/weather_data/output_data/'\n",
    "pred_scaler=load(output_dir+'pred_scaler.joblib')\n",
    "aq_scaler=load(output_dir+'aq_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design Network\n",
    "def network_build(neuron,batch_size,dropout_factor,optimizer):\n",
    "    def fit_lstm(neuron,batch_size,dropout_factor,optimizer):\n",
    "         m = Sequential()\n",
    "         m.add(LSTM(neuron,input_shape=predictors_train_rnn.shape[1:], return_sequences=True, activation='tanh'))\n",
    "         m.add(LSTM(neuron, activation='tanh'))#\n",
    "         m.add(Dropout(dropout_factor))#\n",
    "         m.add(Dense(128, activation='tanh'))\n",
    "         m.add(Dense(f_h))\n",
    "         m.compile(loss='mean_squared_error',optimizer=optimizer)\n",
    "         return m\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "    model=fit_lstm(neuron,batch_size,dropout_factor,optimizer)\n",
    "#  predictors_train_rnn.shape[1:]\n",
    "    \n",
    "    \n",
    "\n",
    "# In[22]:\n",
    "          \n",
    "    def summary_history(m):\n",
    "        print(m.summary())\n",
    "        print(\"Inputs: {}\".format(m.input_shape))\n",
    "        print( \"Outputs: {}\".format(m.output_shape))\n",
    "        history = m.fit(predictors_train_rnn, aq_train_rnn, epochs=70, \n",
    "                        validation_data=(predictors_test_rnn, aq_test_rnn),batch_size=batch_size, verbose=2, shuffle=True)           \n",
    "        return None\n",
    "    summary_history(model)\n",
    "    nns_dir='/mnt/raid2/System/home/mmohan3/weather_data/neural_networks/'\n",
    "    model_json = model.to_json()\n",
    "    with open(nns_dir+\"model\"+str(neuron)+str(batch_size)+str(dropout_factor)+optimizer+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(nns_dir+\"model_\"+str(neuron)+str(batch_size)+str(dropout_factor)+optimizer+\".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "# In[29]:      Performance evaluation part\n",
    "# ### Testing Using Future Days Data\n",
    "# In[31]:\n",
    "## Scale, Predict, and Descale:\n",
    "      #  predictors_future,aq_data_future,aq_input,x_y=feat_ret(nfiles_test,a,15,15,look_back,f_h)\n",
    "    print ('\\n Done getting predictors') \n",
    "    X = list(predictors_future.values())\n",
    "    print(len(X))\n",
    "    X = list(aq_data_future.values())\n",
    "    print(len(X))\n",
    "    \n",
    "    \n",
    "    ## Scale, Predict, and Descale:\n",
    "    \n",
    "    def get_model_predictions(model, X_as_dict, X_scaler, y_scaler):\n",
    "        X = list(X_as_dict.values())\n",
    "        X_temp = [X_scaler.transform(mx) for mx in X]\n",
    "        X_scaled = np.stack(X_temp)\n",
    "        y_scaled = model.predict(X_scaled)\n",
    "        return y_scaler.inverse_transform(y_scaled)\n",
    "    \n",
    "    y_model_predicted = get_model_predictions(model, predictors_future, pred_scaler, aq_scaler)\n",
    "    y = np.stack(aq_data_future.values()).reshape(-1,f_h)\n",
    "    \n",
    "    y_labels=list(aq_data_future.keys())\n",
    "    \n",
    "    y_pred_dict={}\n",
    "    print(y_labels)\n",
    "    for i in range(len(y_labels)):\n",
    "        y_pred_dict[y_labels[i]]=y_model_predicted[i]\n",
    "\n",
    "    output_dir='/mnt/raid2/System/home/mmohan3/weather_data/output_data/'\n",
    "    day_c=[]\n",
    "    d=198\n",
    "    for i in range(no_of_months):\n",
    "        day_c.append(d)\n",
    "        d=d+30\n",
    "    b=[2,7]\n",
    "    loc_c=tuple([58,121])\n",
    "    loc_b=[]\n",
    "    for i in b:\n",
    "        loc_b.append(tuple([loc_c,i]))\n",
    "    \n",
    "    b_loc_day=list(product(day_c,loc_b))\n",
    "    months=['July','August','September','October']\n",
    "    \n",
    "    df=pd.DataFrame()\n",
    "    for i in range(no_of_months):\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        plt.rcParams['axes.linewidth'] = 5\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        colors=['b','g']\n",
    "        df=pd.DataFrame()\n",
    "        \n",
    "        for j in range(2):\n",
    "            y_pred_trend =y_pred_dict[b_loc_day[i * 2 + j]]\n",
    "            y_obs_trend =  aq_data_future[b_loc_day[i*2+j]]\n",
    "            df['aq_input'+str(b_loc_day[i*2+j])]=aq_input[b_loc_day[i * 2 + j]]\n",
    "            df['y_pred_trend'+str(b_loc_day[i*2+j])]=pd.Series(y_pred_dict[b_loc_day[i * 2 + j]])\n",
    "            df['y_obs_trend'+str(b_loc_day[i*2+j])]=pd.Series(aq_data_future[b_loc_day[i * 2 + j]])\n",
    "        fig.show()\n",
    "        ax.set_ylim([0.00,0.09])\n",
    "        ax.set_ylabel('Hr', fontdict={'fontsize':10}, labelpad=20)\n",
    "        ax.set_xlabel('Concentration (ppm)', fontdict={'fontsize':10}, labelpad=20)\n",
    "        ax.set_title('Fig 5b. Model Predictions vs \\n Observed on Test Dataset for a 24 Hr Forecast - '+months[i], fontdict={'fontsize':15},y=1.08)\n",
    "        fig.savefig(output_dir+str(neuron)+str(batch_size)+str(dropout_factor)+'6b-'+str(i)+'.pdf', format='pdf', dpi=1200)\n",
    "    df.to_excel(output_dir+str(neuron)+str(batch_size)+str(dropout_factor)+'output_file.xlsx')\n",
    "    fig=plt.figure(figsize=(8,8))\n",
    "    plt.rcParams['axes.linewidth'] = 5\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    rmse_list=[]\n",
    "    for i in range(no_of_months):\n",
    "        select_points=random.sample(x_y,k=5)\n",
    "        hours=[2,3,6,9]\n",
    "        point_hour=list(product(select_points,hours))\n",
    "        rmse=[]\n",
    "        for j in point_hour:\n",
    "            pred_data=y_pred_dict[tuple([day_c[i],j])]\n",
    "            aq_data=aq_data_future[tuple([day_c[i],j])]\n",
    "            mse=mean_squared_error(pred_data,aq_data)\n",
    "            rmse.append(np.sqrt(mse))\n",
    "        rmse_list.append(rmse)\n",
    "    plt.boxplot(rmse_list)\n",
    "    plt.title('Average RMSE for the first day of JASO months sampled over 5 different coordinates', fontdict={'fontsize':10},y=1.08)\n",
    "    fig.savefig(output_dir+str(neuron)+str(batch_size)+str(dropout_factor)+'7b.pdf', format='pdf', dpi=1200)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[16,32,64,128]\n",
    "batch_sizes=[15,30,45,60,75,90,105,120]\n",
    "dropout_factors=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "optimizers=['adam','RMSProp']\n",
    "\n",
    "for i in batch_sizes:\n",
    "    neuron=64\n",
    "    batch_size=60\n",
    "    dropout_factor=0.5\n",
    "    optimizer='RMSProp'\n",
    "    network_build(i,batch_size,dropout_factor,optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
